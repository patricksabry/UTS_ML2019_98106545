{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricksabry/UTS_ML2019_ID98106545/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4pltkfrUxv",
        "colab_type": "text"
      },
      "source": [
        "# First Impressions\n",
        "\n",
        "What is my chosen paper to read?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "What type of the main contribution the paper has made?\n",
        "\n",
        "A theory or proposition (revealing something, from unknown to known)\n",
        "A method or algorithm (inventing a technique, from undoable to doable)\n",
        "Before reading the main body of the paper, write down your first impression obtained from its abstract and short introduction.\n",
        "\n",
        "Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "\n",
        "Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list, compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "(During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78DedMVasniB",
        "colab_type": "text"
      },
      "source": [
        "# Review on RUM86 - LEARNING INTERNAL REPRESENTATIONS BY ERROR PROPAGATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9jmHevss1Z7",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The following paper is a a critique on the seminal research work conducted by Hinton et al in 1985 on error propagation in artifical neural networks. The paper serves to expound the group's efforts in identifying the success of using back propagation to promote self learning internal representations in deep neural networks. The group's early work on generalised error minimization techniques paved the way for advancements in deep neural networks as we know it today. This paper explores these techniques in detail and reinforces them with tangible examples using current technologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFjdKtjN5Emr",
        "colab_type": "text"
      },
      "source": [
        "# Content \n",
        "\n",
        "In the realm of machine learning the solution domain has been well understood for simple two layer associative networks for quite some time. Such networks allow an input layer of data to be directly mapped to a set of output patterns, meaning there is no internal (hidden) representation to be interpreted. Despite generalising output patterns reasonably, these simple networks are beholden to the rules encoded into the input data by external forces and as such are unable to learn unorthodox mappings from input to output without hidden units. The paper explores the limitations of traditional gradient descent in two layer systems in solving more complicated problems such as the XOR problem, and how such problems can be solved through the learning of internal representations using more a sophisticated, generalised gradient descent rule.\n",
        "\n",
        "The research area focuses on extending the detla rule for semilinear activation functions in feedforward neural networks and proposes a new error propagating algorithm called the generalised delta rule. The main problem with standard detla rule was its inability to confidently compute error derivatives in non concave error space. Convoluted error spaces are common in deep neural networks compared to two layer networks which typically have a parabolic representation in error space (##use reference here to back this up).\n",
        "\n",
        "The generalised delta rule solves the problem of local minima in a wide variety of arbitrary networks where the standard delta rule would otherwise fail due to non linear error surfaces. The proofing of the generalised delta rule in this paper was imperative to the drastic advancements in deep neural network architectures in the subsequent years. The success of all deep neural networks is heavily predicated on the power and reliability of generalised gradient descent algorithms in minimizing cost functions across a plethora of problem domains.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWO1hXScn4aU",
        "colab_type": "text"
      },
      "source": [
        "# Innovation\n",
        "\n",
        "* How innovative is the research work in the paper? describe the 'novelty' of the paper\n",
        "* What does the paper contribute? a new method/ algorithm/ methodology? comparison between different methods\n",
        "\n",
        "Although not the first to propose the use of backpropagation to train neural networks, Hinton et al's paper published in 1985 played an integral role in popularizing the use of backpropagation to train multi-layer neural networks reliably. The novelty of the paper was ultimately its ability to bridge the knowledge gap between training simple two-layer networks and multi-layer networks. The issues with the previously widely accepted delta rule\n",
        "\n",
        "\n",
        "\n",
        "* talk about simple feedforward networks --> transition to efficient multi-layer networks getting trained efficiently\n",
        "* Example: linear cost function tuning example from 2 layer NN (parabolic cost function)\n",
        "* Example: non-linear 3D space cost function tuning example from multi-layer NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhL3il8aoW_1",
        "colab_type": "text"
      },
      "source": [
        "# Technical Quality\n",
        "\n",
        "* How would you rate the technical quality of the work in the paper? Here you should consider the quality of the work done.\n",
        "\n",
        "The paper defines the generalised detlas rule as\n",
        "\n",
        "\n",
        "$\\Delta$p$w_{ji}$ = $\\eta$($t_{pj}$ - $o_{pj}$) $i_{pi}$ = $\\eta$$\\delta_{pj}$$i_{pi}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NyEiPPhorFV",
        "colab_type": "text"
      },
      "source": [
        "# Application and X-factor\n",
        "\n",
        "Do you think the application domain is appropriate for the proposed technique? What other\n",
        "application domains could the research work be applied? Also in this section, give a couple of suggestions for further\n",
        "developments of the research work. Do you think the work described in the paper could spark a good discussion in\n",
        "class? What did you find interesting about the work? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuxV_dF9TeeG",
        "colab_type": "text"
      },
      "source": [
        "# Presentation\n",
        "\n",
        "* Talk about lack of summary of notations. Would be good to have a summary of notations to allow for easier interpretation of formulas in the paper especially considering the extensive use of formulas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVlnMskFonMN",
        "colab_type": "text"
      },
      "source": [
        "# References \n",
        "\n",
        "list of references used in this paper."
      ]
    }
  ]
}