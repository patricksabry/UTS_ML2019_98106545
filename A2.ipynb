{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricksabry/UTS_ML2019_ID98106545/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdbXy2nbjVlM",
        "colab_type": "text"
      },
      "source": [
        "# Scalable Recommendation System using ALS in Spark (PySpark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfny356y0zzP",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In the current age of information recommendation systems are a cornerstone of majority of our online interactions with products and social networks. Hyper consumption of media and consumer products online has seen machine learning powered recommendation systems rise to prominence on platforms such as Amazon, Facebook and Netflix. By leveraging behavioral data and supervised machine learning, internet users are being served tailored, personalised experiences when shopping and consuming online media. The widespread acknowledgment of the utilitarian value-add that recommendation systems provide has led to mercurial advancements in scalable compute frameworks and parallelized machine learning techniques. One of the most notable of these is Spark and Spark MLlib, a fast and scalable unified analytics engine for big data processing and machine learning.\n",
        "\n",
        "The following notebook outlines the implementation of a content based recommendation system using Apache Spark. Spark will be used for scalable parallel memory-based data processing. This will ensure that this algorithm implementation is highly scalable based on how large of a compute cluster it is run on. Spark's ML library will be accessed via PySpark, Spark's high level Python API, and the dataset used for the recommendation system will be the open sourced MovieLens dataset. Movies will be recommended based on collaborative filtering using an Alternate Least Squares algorithm to find similar user ratings for certain movies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOn4q1gjkSz",
        "colab_type": "text"
      },
      "source": [
        "# Social and Ethical Implications of recommendation systems\n",
        "\n",
        "Utilitarian ethics suggests that the actions of an agent or system should be taken in the best interest of happiness and wellbeing. An ethical framework predicated on virtuous value maximization is befittingly adopted when developing this recommendation system. Recommendation systems serve to create value for users of a system by intelligently suggesting items such as products or movies to a user based on the historical interactions of similar users with the system. It is plausible to suggest that the utility afforded by the recommendation system to the users outweighs the privacy concerns of collection and manipulation of behavioral user data.\n",
        "\n",
        "In the context of this particular system, the dataset used for training the model is open sourced for academic use, abolishing concerns of privacy exploitation. When collecting and cleaning behavioral user data I believe it is imperative to clearly communicate the intention of data use with the users and model the data in a meaningful way to mutually benefit the user and the platform in question. This can be communicated through end user license agreements and terms of use, which is evident on platforms such as Amazon and Netflix.\n",
        "\n",
        "Misplaced trust of users in a given platform's privacy policies can have detrimental social and ethical implications. This was highlighted in the Cambridge Analytica Facebook scandal, whereby personal user data was wrongfully collected and used to engineer a politically biased advertisement recommendation campaign which was used to push a political agenda. This misuse of data ultimately resulted in Facebook incurring a $5 billion fine in July 2019.\n",
        "\n",
        "Evidently, intentionally malicious misuse of user data in building a recommendation system such as in this project should be avoided at all costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9bOh0fE1xF8",
        "colab_type": "text"
      },
      "source": [
        "# Data Exploration and Pre-processing\n",
        "\n",
        "This ALS algorithm implementation will leverage the distributed processing power of spark through the use of Resilient Distributed Datasets (RDD). The Apache foundation defines an RDD as an immutable distributed collection of objects. Each dataset is divided into logical partitions, which may be computed on different nodes of the cluster. This implementation is highly scalable and boasts data read speeds of up to 100x faster than Hadoop MapReduce.\n",
        "\n",
        "In order to pre-process the MovieLens data it must first be imported and transformed into a spark dataframe. The dataset is comprised of two independent files 'movies.csv' and 'ratings.csv' which are hosted on this project's GitHub repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90C_MDSQJCYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('expand_frame_repr', False) \n",
        "\n",
        "movies_url = 'https://raw.githubusercontent.com/patricksabry/UTS_ML2019_ID98106545/master/movies.csv'\n",
        "ratings_url = 'https://raw.githubusercontent.com/patricksabry/UTS_ML2019_ID98106545/master/ratings.csv'\n",
        "df_movies = pd.read_csv(movies_url)\n",
        "df_ratings = pd.read_csv(ratings_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etNSGc9qk5Dd",
        "colab_type": "text"
      },
      "source": [
        "The `movies` and `ratings` dataframes must be represented as RDDs of triplets in the following format:\n",
        "\n",
        "$R$ $\\rightarrow$ RDD(($u$, $i$, $r_{ui}$), . . .)\n",
        "\n",
        "With each row consisting of a rating $r_{ui}$ given to item $i$ by user $u$. This matrix multiplication results in a highly sparse consolidated matrix with users and items as column and row indexes respectively, and explicit item ratings as tuples. This matrix will contain null ratings due to the high sparsity of the dot product, and thus the ultimate goal of the ALS algorithm is to fill in these null values with predicted ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3vDFKIMMYnd",
        "colab_type": "code",
        "outputId": "e29b0a13-af3d-43d9-d57a-4fdf7791b4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df = pd.merge(df_ratings, df_movies, on='movieId')\n",
        "print(df.head())\n",
        "\n",
        "num_ratings = df.rating.count()\n",
        "num_movies = df['movieId'].nunique()\n",
        "num_users = df['userId'].nunique()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Observed \" + str(num_ratings) + \" ratings from \" + str(num_users) + \" users on \" + str(num_movies) + \" movies\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   userId  movieId  rating   timestamp             title                                       genres\n",
            "0       1        1     4.0   964982703  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
            "1       5        1     4.0   847434962  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
            "2       7        1     4.5  1106635946  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
            "3      15        1     2.5  1510577970  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
            "4      17        1     4.5  1305696483  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
            "\n",
            "\n",
            "Observed 100836 ratings from 610 users on 9724 movies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMcei_NjtJg",
        "colab_type": "text"
      },
      "source": [
        "# Implementation Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPwUNUHpCZwy",
        "colab_type": "text"
      },
      "source": [
        "The system will strive to learn latent factors by minimising an error measure in respect to a source truth ratings matrix using the Alternate Least Squares (ALS) algorithm which will be fitted on the pre-processed data.\n",
        "\n",
        "The ALS algorithm can be defined as\n",
        "\n",
        "$\\hat{r}$ = $\\sum_{f=0}^{n factors}$ $H_{u,f}$$W_{f,i}$\n",
        "\n",
        "Whereby for any item $i$ given by user $u$ the rating of the item can be expressed as a matrix dot ptoduct of the user latent vector $H$ and the item latent vector $W$.\n",
        "\n",
        "The ALS algorithm achieves this by factoring the matrix $R$ into two constituent matrices $U$ and $P$. These constituent matrices are initialised with random values, and when multiplied together they should form an approximation of the source truth ratings matrix $R$. Subsequently, the algorithm proceeds to calculate an error term on the product of the latent factor matrices $U$ and $P$ in respect to the original ratings matrix $R$. This error adjustment is achieved by alternating between the factor matrices and adjusting their values until the error measure is minimized. Once this training process is completed, the product of the factor matrices could be used to infer item recommendations to users, and vice versa users to particular items.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s4qobgK5aiH",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Spark dependencies & initializing enviornment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YJQG-6uy2sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thDdgQ2B0dy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set java sdk and Spark enviornment paths\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\"\n",
        "\n",
        "# Use PySpark to intialise spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbZoXFg1Yhc_",
        "colab_type": "text"
      },
      "source": [
        "## Building and optimizing the ALS model using Cross-fold Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgM-TKuyYLSb",
        "colab_type": "code",
        "outputId": "9e95370e-c4da-4fc8-a59b-3c1ac8297ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import time\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "# Convert Pandas dataframe to Spark RDD based dataframe\n",
        "spark_df = sc.createDataFrame(df)\n",
        "spark_df_movies = sc.createDataFrame(df_movies)\n",
        "\n",
        "# Split the input data into train, validation and test datasets \n",
        "X_train, X_val = spark_df.randomSplit([0.7,0.3])\n",
        "\n",
        "# Instantiate baseline ALS, configured to drop NaN values in factors and refute negative predictions\n",
        "# to avoid evaluation metric polution\n",
        "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative=True, coldStartStrategy='drop')\n",
        "\n",
        "# Define gridsearch parameters to apply to the ALS model\n",
        "# Note: As this application of ALS is explicit, the alpha parameter is ignored\n",
        "param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(als.rank, [6,8,10]) \\\n",
        "            .addGrid(als.maxIter, [5,8,10]) \\\n",
        "            .addGrid(als.regParam, [0.01,0.1,0.14,0.18]) \\\n",
        "            .build()\n",
        "\n",
        "# Define evaluator to measure RMSE loss in cv training\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "\n",
        "# Instantiate cross validation using 3-fold cv\n",
        "# If time permits, 5-fold validation should be used instead for optimal results\n",
        "start = time.time()\n",
        "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Fit ALS model to train data\n",
        "model = cv.fit(X_train)\n",
        "\n",
        "end = time.time()\n",
        "gridsearch_time_elapsed = (end - start)\n",
        "\n",
        "# Extract best model from gridsearch \n",
        "best_model = model.bestModel\n",
        "\n",
        "# Generate predictions and evaluate them against test data subset\n",
        "predictions = best_model.transform(X_val)\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "# Expose evaluation metrics and best model parameters\n",
        "print(\"Best model achieved an RMSE of \" + str(round(rmse,4)) + \" with parameters [\" \n",
        "      + \"maxIter: \" + str(best_model._java_obj.parent().getMaxIter()) + \" ranks: \" \n",
        "      + str(best_model.rank) + \" regParam: \" + str(str(best_model._java_obj.parent().getRegParam())) + \"]\")\n",
        "\n",
        "print(\"Gridsearching best model took \" + str(round(gridsearch_time_elapsed,2) + \"seconds\")            \n",
        "\n",
        "# Generate top 10 movie recommendations for each user\n",
        "userRecs = best_model.recommendForAllUsers(10)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best model achieved an RMSE of 0.8841 with parameters [maxIter: 10 ranks: 10 regParam: 0.18]\n",
            "Gridsearching best model took 883.6228220462799seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA6hTgMxkFmA",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "The recommendation model's performance was evaluated by comparing baseline and gridsearch-tuned models across a number of different model configurations. This included testing different train/test splits and different error measures to determine the most optimal model to use.\n",
        "\n",
        "Train | Test | Model | FeatureSet | Measure | Value | IsBaseline\n",
        "--- | --- | --- | --- | --- | --- | ---\n",
        "X_train 70% |X_val 30% |ALS | RDD(user, item, rating), default | RMSE | -- | 1\n",
        "X_train 70% |X_val 30% |ALS | RDD(user, item, rating), default | MAE | -- | 1 \n",
        "X_train 80% |X_val 20% |ALS | RDD(user, item, rating), default | RMSE | -- | 1\n",
        "X_train 80% |X_val 20% |ALS | RDD(user, item, rating), default | MAE | -- | 1\n",
        "X_train 70% |X_val 30% |ALS | RDD(user, item, rating), 3-fold CV gridsearch | RMSE | 0.8841 | 0\n",
        "X_train 70% |X_val 30% |ALS | RDD(user, item, rating), 3-fold CV gridsearch | MAE | -- | 0\n",
        "X_train 80% |X_val 20% |ALS | RDD(user, item, rating), 3-fold CV gridsearch | RMSE | 0.8841 | 0\n",
        "X_train 80% |X_val 20% |ALS | RDD(user, item, rating), 3-fold CV gridsearch | MAE | -- | 0\n",
        "\n",
        "\n",
        "\n",
        "## Time and space complexity\n",
        "* discuss time() measurements of gridsearch and real-time prediction output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVJOPrQbwo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to fetch and display a given user's original movie rating behaviour (input data)\n",
        "def get_original_user_ratings(user_id):\n",
        "    ratings_df = spark_df.select(\"movieId\", \"rating\", \"title\", \"genres\")\n",
        "    ratings_df = spark_df.filter(spark_df.userId == user_id)\n",
        "    \n",
        "    df = ratings_df.toPandas()\n",
        "    df = df.sort_values(['rating'], ascending=[False])\n",
        "    df = df[0:10]\n",
        "    print(\"Original Ratings provided by User \" + str(user_id))\n",
        "    print(df)\n",
        "    \n",
        "# Utility function for `format_user_recommendations` function which returns movie recommendations\n",
        "# as well as movie attributes such as title and genres for eaier interpretation of data\n",
        "def get_user_recommendations(recs):\n",
        "    recs = recs.select(\"recommendations.movieId\", \"recommendations.rating\")\n",
        "    movies = recs.select(\"movieId\").toPandas().iloc[0,0]\n",
        "    ratings = recs.select(\"rating\").toPandas().iloc[0,0]\n",
        "    ratings_matrix = pd.DataFrame(movies, columns=[\"movieId\"])\n",
        "    ratings_matrix[\"ratings\"] = ratings\n",
        "    ratings_matrix_ps = sc.createDataFrame(ratings_matrix)\n",
        "    \n",
        "    return ratings_matrix_ps\n",
        "\n",
        "# Function to display recommended movies for a given user, including the predicted rating scores\n",
        "def format_user_recommendations(user_id):\n",
        "    # Filter dataframe to only return current user's movie recommendations\n",
        "    user_df = userRecs.filter(userRecs.userId == user_id)\n",
        "    df = filteredDf.toPandas()\n",
        "    final_recs = get_user_recommendations(user_df)\n",
        "    ratings_df = final_recs.toPandas()\n",
        "    movies_df = spark_df_movies.toPandas()\n",
        "    formatted_matrix = pd.merge(ratings_df, movies_df, on='movieId')\n",
        "    print(\"Movie recommendations for User \" + str(user_id))\n",
        "    print(recommendations)\n",
        "    \n",
        "\n",
        "get_original_user_ratings(14)\n",
        "print(\"\\n\")\n",
        "format_user_recommendations(14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COL7RORIkCjA",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "* Discuss lack of computing power\n",
        "* Discuss lack of data (using 100k dataset instead of 1M, 10M etc due to lack of resources)\n",
        "* Discuss limitation of explicit colaborative filtering. The collection of data is contingent on user interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi17lEHclzYf",
        "colab_type": "text"
      },
      "source": [
        "# References \n",
        "\n",
        "http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf\n",
        "\n",
        "https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal#Use_of_the_data\n",
        "\n",
        "https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm\n",
        "\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.recommendation\n",
        "\n",
        "https://core.ac.uk/download/pdf/76649438.pdf\n",
        "\n",
        "http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/collaborativefiltering.html\n"
      ]
    }
  ]
}