{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricksabry/UTS_ML2019_ID98106545/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfny356y0zzP",
        "colab_type": "text"
      },
      "source": [
        "# Scalable Recommendation System using Spark (PySpark)\n",
        "\n",
        "The following notebook outlines the implementation of a content based recommendation system using Apache Spark. Spark will be used for scalable parallel memory-based data processing. This will ensure that this algorithm implementation is highly scalable based on how large of a compute cluster it is run on. Spark's ML library will be accessed via PySpark, Spark's high level Python API.\n",
        "\n",
        "The dataset used for the recommendation system will be the open sourced MovieLens dataset. Movies will be recommended based on collaborative filtering using an Alternate Least Squares algorithm to find similar user ratings for certain movies.\n",
        "\n",
        "$\\hat{r}$ = $\\sum_{f=0}^{n factors}$ $H_{u,f}$$W_{f,i}$\n",
        "\n",
        "Whereby for any item $i$ given by user $u$ the rating of the item can be expressed as a matrix dot ptoduct of the user latent vector $H$ and the item latent vector $W$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9bOh0fE1xF8",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset & exploration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s4qobgK5aiH",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Spark dependencies & initializing enviornment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YJQG-6uy2sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thDdgQ2B0dy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XquY2IkQzUWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}